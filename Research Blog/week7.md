## Week 7

### <u>Day 43 (July 17th)</u>

Week 7, Day 1

 - 8:30 - 9:00 AM: Wow. It's week 7 already, which is crazy. Time is dwindling -- I hope we get some cool research results to present in just under three weeks. Instead of getting out for my Monday morning swim, I played it safe and waited for AG, the grad lead for REU-CAAR, to bring all the isolating students COVID Test Kits (which is quite nice of him). I tested Negative and so that meant I could finally get back to work at IRB. I broughout out my mask just to be safe as well.

 - 10:00 AM - 12:00 PM: To start off the research week, I wanted to work on testing the results from the generated images (from our diffusion models) on the DeepFace classifier. After some experimentation with prompting and comparing the results to work from my other group members, I generated 15 images for each of the four types of prompts (portrait/group images of doctors/firefighters) and proceeded to hand-label the portrait images. I then used the image to classification pipeline that I had written from previous weeks, modified some of the code, and ran it on the generated images. To analyze the results, I would need to hand-label the images (since they are generated and are not part of a labeled dataset). After some discussion with my group, I ended up doing this for the portrait images before heading to lunch.

 - 12:00 - 1:30 PM: We had another one of our weekly Monday REU lunch talks. Today Bill gave a talk on [The Hat Problem](https://www.cs.umd.edu/~gasarch/REU/hatstalk.pdf). I had seen this problem before but did not remember the strategy (and I also had not seen the many extensions proposed by Bill); overall, I found the talk quite enjoyable and it was great to think mathematically/problem-solve for the first time in a while. I definitely want to ask Bill whether I can present a version of this to professors at Swarthmore, or even present it as part of the IMC (Interactive Math Colloquium) Club I run, since I very much enjoyed this talk!

- 1:30 - 4:00 PM: I quite enjoyed Bill's Hat Talk so I spent some time afterwards discussing the proofs (especially the infinite hat, 2 colors and infinite hat, at most 1 error cases) with AF and SS (XG, our final group member, is going to be working remotely this week). We talked a bit about the [Axiom of Choice](https://en.wikipedia.org/wiki/Axiom_of_choice), our thoughts on it, as well as some resulting paradoxes including Banach–Tarski (the Axiom of Choice is used in one of the proofs discussed by Bill). Afterwards, I ended up discussing the general research plan (what everyone was working on, potential limitations of our research and what we can do to address them, and what we can do next to get a solid research project before the REU ends) with my group members AF and SS. I think this was really productive -- at times, I feel like I've only been working on a small part of the pipeline, but it's important to realize that the general fairness process we are trying to build relies on many different small but essential parts. AF and I also tried to reason through mathematical foundation behind a Latent Diffusion Model paper SS was trying to read through, to some success. I didn't get too much done outside of this group discussion, but I feel like having these sorts of discussions is really necessary to get everyone aligned and on the same page and to build a solid working dynamic, so I was quite happy with that. 

After research, I ended up relaxing a bit before heading out to "play badminton" with ZF, AG, and SS. It turns out there were only 3 badminton racquets, and since I didn't want them to have to rotate, I ended up playing Racquetball on my own for 30ish minutes. Afterwards, I waited around and watched them and others play badminton. This ended up taking much longer than anticipated, which I was okay with since I mainly was on my phone after a while, but it meant that we had to jog to CFA before it closed at 7. Obviously, then, since we ordered right before closing, the food wasn't as good as normal. Oh well.

Outside of research, I've been waiting for my monitor to arrive (it was supposed to arrive yesterday but the delivery didn't go through), and it hasn't arrived yet today, either, which kind of sucks. I'm really looking forward to a better setup to hopefully allow me to be more productive and work easier in my room. I've also been waiting to hear from the [Young Mathematicians Conference](https://ymc.osu.edu/), a math conference I applied to using my work from last year, so fingers crossed on that front! 


### <u>Day 44 (July 18th)</u>

Week 7, Day 2

 - 8:30 - 9:00 AM: I had planned to exercise outside yesterday (swim or run) and in the morning today, but we hit another wave of bad air quality (I think this is the third one in this REU program alone, with 130+ AQI, meaning the Air Quality is unhealthy for outdoor activity). When I woke up at 6:30 AM to check on the AQI, it was still Unhealthy, so I set my alarms for 9 to get an extra hour of sleep.

 - 10:00 AM - 12:00 PM: Over the last few days, I've been focused on trying to get good diffusion model output as well as trying to test the DeepFace classifier on the generated output. For the latter part, I have everything I need to do this (I would need to write a bit more code to do so but it wouldn't take too long), but I have been putting it off. My reasoning for this is twofold: many of the generated group images have terrible image quality (faces are deformed), making labeling and comparing to the results from the classifier difficult/not useful. Furthermore, even for the single images (one person in an image), the hand-labeling of a person's race is very subjective and thus it's difficult to accurately analyze the classifier. As a result, I've moved away from this and have thought more about how we can get fair image generation. One of our current projects, led by AF, is Domain Translation, which involves masking a certain part of an image (e.g. one person) and using a Blended Latent Diffusion model to "translate" that person's gender or race. For example, one can mask a man and say "change this person to a woman." Ideally, this person would look very similar characteristic wise as to what the original person is wearing, since the model tries to "blend" the result into the rest of the image. Through some initial experiments, this has been unsuccessful/difficult. As a result, I've been looking into some work that follows a [Semantic Guidance](https://arxiv.org/pdf/2301.12247.pdf) paper, where given an image and a concept, the concept is added/suppressed in the image while preserving the other characteristics of an image.

 - 1:00 - 4:00 PM: In the morning, I read a bit about the paper and the general process before starting to implement the code from the [Semantic Guidance Colab](https://colab.research.google.com/github/ml-research/semantic-image-editing/blob/main/examples/SemanticGuidance.ipynb#scrollTo=4e6e6724) locally. I got this working and started looking at generating group images and how guidance on gender/race would affect the image. Tomorrow, I want to try to combine our existing domain translation approach (where a certain part of the image is masked) and the Semantic Guidance approach. My (maybe optimistic) hope is to be able to mask a certain part of the image, guide that part of the image, and "stitch" the resulting parts together to lead to a fair outcome, as this will preserve general background characteristics in a generated image while making the image more fair. We closed the day by talking a bit as a group, and we somehow ended up working on some math problems from China's famous Gaokao examination (which was fun) as well.

 After research, I headed to the gym to play basketball with PM, MA, AB, SS, and DS before joining AG and ZS at the pool afterwards. We mainly messed around at the pool (there was so many people and the lanes were all packed), which included me and ZS attempting to teach SS how to dive into a pool as well as going on the slide/diving boards. I also briefly swam a few laps near the end, when lanes started to free up. I got home back around 8 PM, warmed up some leftover pizza, did some CodeForces, and that was that. 

 For some non-research related content, I had to cancel my monitor shipment from Amazon today, since it has not been delivered despite remaining at the Amazon facilities. This kind of sucks since I was hoping to work on a monitor for the remainder of the REU (and I will hand it off to Sherry post research for her to use in industry). On the plus side, I found out today I was accepted as a Report Presenter at this year's YMC (Young Mathematicians Conference), which I am quite happy about! I guess maybe not getting a monitor for now is a small price to pay for getting accepted into YMC. :)

### <u>Day 45 (July 19th)</u>

Week 7, Day 3

 - 8:30 - 9:00 AM: AQI is back to normal and I'm out of isolation so there are no excuses for not going for a morning swim now! I, admittedly reluctantly, got out of bed and over to the pool for a quick 25 minute, 40 lap swim. It's good to get back to lap swimming and hopefully I'll get back into the groove of things quickly.

 - 10:00 AM - 12:00 PM: In the morning, I experimented a bit more with the Semantic Guidance Pipeline from diffusers, specifically the prompt manipulation which leads to a different generated image. A minor yet useful adjustment I made was displaying multiple images in a grid-like fashion, so I can compare the results before/after each guidance stage and compare it to the original image output. It seems like there's a difference between guiding "male" and reverse-guiding "female," so we may need to think about which one we want if there is an abundance of one gender (for example, guiding "male" may generate a few men in places where the original image had no person -- this may not be desirable).

 - 1:00 - 4:00 PM: I started to test the Semantic Guidance pipeline a bit more on group images. I think I will need to tinker around more with the other hyperparameters (including the number of editing steps, guidance strength, edit momentum, etc.) to get a handle of the parameters we would want to use to lead to more fair image generation. Adding the reverse prompting of "deformed, blurry, ..." and other negative prompts I used in stable diffusion experimentation led to slightly better quality images most of the time, but when I used these prompts without the additional guidance of a gender term, the model started generating almost exclusively people with darker skin and doctor hats on... which is weird. I'll continue experimenting with this to see why this is the case. Also, I have been thinking that we will need to use Semantic Guidance in combination with a mask, but I don't think this is the case anymore -- especially if this alone can lead to more fair image generation for groups, which would itself be novel; the masking approach is more of a postprocessing than the one I am currently trying.

 After research, I hung around in our work room to get a little bit more work done, type up this, and then head down for our second Board Game night of the REU. I started by joining a mini game of SET before starting a new game of Olympus with DS and Bill Gasarch, the program director. After a bit of a learning curve and Bill teaching us how to play, we successfully completed a game, with Bill finishing first with 51 points, me finishing second with 48, and DS finishing in last with 31 (which was hilarious since he was joking about how he wanted to finish ahead of me). In between, we paused for some catering from a nearby Indian place, which was much tastier than I had expected. I closed out the night by briefly playing some Poker and some Codenames with some graduate students as well as SS, AZ, DS, MA, LH, and ML. 

 There is no meeting with our advisor this week since she is heading to Honolulu for [ICML (International Conference on Machine Learning)](https://icml.cc/), but we plan to update her on Slack with our weekly progress.

### <u>Day 46 (July 20th)</u>

Week 7, Day 4

 - 8:30 - 9:00 AM: Once again, I got out of bed reluctantly to head out for a morning swim. I have been sleeping closer to midnight these last few days and might have to sleep a bit earlier to be more energized for these swims... Anyways, I ended up going for 30 minutes and 50+ laps, which is great pace-wise especially considering I'm just coming back from a break due to isolation and bad air quality.

 - 10:00 AM - 12:00 PM: On Thursday mornings, we usually have grad meetings with my advisor's group, but everyone is out for the previously mentioned ICML conference; as a result, the next two grad meetings are canceled. This means a bit more work time on Thursday mornings, and so today I continued to work a bit more on Semantic Guidance. I am hoping to solve individual fairness first (similar to other papers, like Fair Diffusion) but apply my own touch for the approach. As a result, I also spent some of the morning looking through the Fair Diffusion paper's code and thinking about where I can extend their procedure/improve on it.

 - 1:30 - 4:00 PM: In the afternoon session, I generated and modified a few images using SEGA (Semantic Guidance) but mainly focused on writing an update for my mentor (our updates to her will take the place of our meeting this week). Here are my thoughts, split into 1) limitations of our project/approaches so far and 2) what I've been working on this week.

  > As we talked about in last week’s group meeting, we have been trying to think about some of the limitations of our current approaches. It is difficult to say how we can deal with them, but we are trying to answer them/keep them in mind as we work on our different approaches:
    <li>For the generative process, we find that stable diffusion generated images yield deformed/blurry faces for group images. Ideally, we want these to be higher quality/more human like (as this might make our classifiers more accurate resulting in higher fairness).</li>
    <li>In our general pipeline, we acknowledge that our gender/race classifier (e.g. DeepFace) may itself be biased/faulty, yet it is a part of our general process for fair image generation.</li>
    <li>We have not figured out how to separate fairness across different concepts (doctors and nurses). For example, if a generated image has 5 male doctors and 5 female nurses, our RL approach may deem this as a “fair image” when it is technically not.</li>

> On the personal side, here’s a bit more about what I’m now working on: last week I tried experimenting with negative prompting in stable diffusion and completed prompt-to-image and image-to-classification pipelines. This week I’ve been focusing on experimenting with SEGA (“Semantic Guidance”), a technique that uses semantic guidance to “guide” diffusion model output. This is similar to the domain translator approach Amy has been working on, but the domain translator modifies the generated diffusion image, whereas SEGA guides the diffusion process by modifying vectors in the semantic space. My current goals for SEGA include getting fair image outputs over a batch for photos with just one person. To extend previous work, I hope to work more with the editing and guidance hyperparameters (for example, in group fairness, I may want to adjust the guidance scales based on the confidence of the race classifiers) as well as use gender and race classifiers to guide the editing process. In comparison, previous papers like Fair Diffusion support prompts of just one format “a photo of the face of a [occupation]” and the guidance is done manually by the user. Though I plan to start by working to solve individual person fairness, I hope to extend these ideas to group fairness as well.

- 4:00 - 5:30 PM: I went to a talk by Professor Victor Yakovneko (Physics Professor at UMD) on "The Statistical Mechanics of Money." The talk was about how the Boltzmann-Pareto distribution (which is typically used to describe the physics of objects in a system) can be similarly applied to monetary concepts such as income/tax flow. I thought the talk itself was pretty interesting, but it ended up going quite a bit over, which was bad since I really had to use the restroom. 

After research activities, I went straight to the gym to play basketball. MA and I arrived first, to no available balls and no available courts. After some clever finagling and asking around, we ended up with both a half-court to play on and a basketball, and so we got a game going with SK, SS, MA, PM, DS, and later, two other students. For dinner, SS let me have one of her Shin Ramen packs, and I had that along with some leftovers from PE from two days before. Afterwards, I showered up, confirmed my participation in YMC this year (yay!), and did some CodeForces to close out the night.